{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from \"tracks/train\\**\\*.wav\"\n",
      "Using default device: cpu\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "from dataset import Dataset\n",
    "from model import SpectrogramModel\n",
    "ds = Dataset(\"tracks/train\")\n",
    "model = SpectrogramModel(len(ds.label_encoder.classes_))\n",
    "trainer = Trainer(ds,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_from(model, name=\"model172.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training 100 epochs, logging every 1, saving every 10, batch size 200.\n",
      "epoch=0 batch=1/1 loss=0.59913\n",
      "epoch=0 batch=2/2 loss=0.66124\n",
      "epoch=1 batch=1/2 loss=0.59546\n",
      "epoch=1 batch=2/2 loss=0.64096\n",
      "epoch=2 batch=1/2 loss=0.59004\n",
      "epoch=2 batch=2/2 loss=0.61685\n",
      "epoch=3 batch=1/2 loss=0.58268\n",
      "epoch=3 batch=2/2 loss=0.58738\n",
      "epoch=4 batch=1/2 loss=0.56254\n",
      "epoch=4 batch=2/2 loss=0.55834\n",
      "epoch=5 batch=1/2 loss=0.53912\n",
      "epoch=5 batch=2/2 loss=0.52913\n",
      "epoch=6 batch=1/2 loss=0.52457\n",
      "epoch=6 batch=2/2 loss=0.49666\n",
      "epoch=7 batch=1/2 loss=0.49505\n",
      "epoch=7 batch=2/2 loss=0.46894\n",
      "epoch=8 batch=1/2 loss=0.47591\n",
      "epoch=8 batch=2/2 loss=0.43960\n",
      "epoch=9 batch=1/2 loss=0.45144\n",
      "epoch=9 batch=2/2 loss=0.42182\n",
      "epoch=10 batch=1/2 loss=0.43689\n",
      "epoch=10 batch=2/2 loss=0.40584\n",
      "epoch=11 batch=1/2 loss=0.42358\n",
      "epoch=11 batch=2/2 loss=0.39252\n",
      "epoch=12 batch=1/2 loss=0.41746\n",
      "epoch=12 batch=2/2 loss=0.37823\n",
      "epoch=13 batch=1/2 loss=0.40959\n",
      "epoch=13 batch=2/2 loss=0.37032\n",
      "epoch=14 batch=1/2 loss=0.40134\n",
      "epoch=14 batch=2/2 loss=0.37745\n",
      "epoch=15 batch=1/2 loss=0.39408\n",
      "epoch=15 batch=2/2 loss=0.35445\n",
      "epoch=16 batch=1/2 loss=0.39663\n",
      "epoch=16 batch=2/2 loss=0.35114\n",
      "epoch=17 batch=1/2 loss=0.38823\n",
      "epoch=17 batch=2/2 loss=0.34789\n",
      "epoch=18 batch=1/2 loss=0.38196\n",
      "epoch=18 batch=2/2 loss=0.36180\n",
      "epoch=19 batch=1/2 loss=0.39184\n",
      "epoch=19 batch=2/2 loss=0.36372\n",
      "epoch=20 batch=1/2 loss=0.38593\n",
      "epoch=20 batch=2/2 loss=0.35813\n",
      "epoch=21 batch=1/2 loss=0.37672\n",
      "epoch=21 batch=2/2 loss=0.34926\n",
      "epoch=22 batch=1/2 loss=0.37167\n",
      "epoch=22 batch=2/2 loss=0.36349\n",
      "epoch=23 batch=1/2 loss=0.37042\n",
      "epoch=23 batch=2/2 loss=0.34965\n",
      "epoch=24 batch=1/2 loss=0.36730\n",
      "epoch=24 batch=2/2 loss=0.35775\n",
      "epoch=25 batch=1/2 loss=0.36505\n",
      "epoch=25 batch=2/2 loss=0.35612\n",
      "epoch=26 batch=1/2 loss=0.36229\n",
      "epoch=26 batch=2/2 loss=0.34868\n",
      "epoch=27 batch=1/2 loss=0.35991\n",
      "epoch=27 batch=2/2 loss=0.34512\n",
      "epoch=28 batch=1/2 loss=0.35913\n",
      "epoch=28 batch=2/2 loss=0.34360\n",
      "epoch=29 batch=1/2 loss=0.35448\n",
      "epoch=29 batch=2/2 loss=0.34019\n",
      "epoch=30 batch=1/2 loss=0.34997\n",
      "epoch=30 batch=2/2 loss=0.32987\n",
      "epoch=31 batch=1/2 loss=0.34528\n",
      "epoch=31 batch=2/2 loss=0.32010\n",
      "epoch=32 batch=1/2 loss=0.33742\n",
      "epoch=32 batch=2/2 loss=0.31622\n",
      "epoch=33 batch=1/2 loss=0.32662\n",
      "epoch=33 batch=2/2 loss=0.29799\n",
      "epoch=34 batch=1/2 loss=0.31428\n",
      "epoch=34 batch=2/2 loss=0.27891\n",
      "epoch=35 batch=1/2 loss=0.29841\n",
      "epoch=35 batch=2/2 loss=0.26194\n",
      "epoch=36 batch=1/2 loss=0.26903\n",
      "epoch=36 batch=2/2 loss=0.36312\n",
      "epoch=37 batch=1/2 loss=0.32055\n",
      "epoch=37 batch=2/2 loss=0.24485\n",
      "epoch=38 batch=1/2 loss=0.25081\n",
      "epoch=38 batch=2/2 loss=0.20068\n",
      "epoch=39 batch=1/2 loss=0.22375\n",
      "epoch=39 batch=2/2 loss=0.19624\n",
      "epoch=40 batch=1/2 loss=0.16692\n",
      "epoch=40 batch=2/2 loss=0.19105\n",
      "epoch=41 batch=1/2 loss=0.26434\n",
      "epoch=41 batch=2/2 loss=0.23996\n",
      "epoch=42 batch=1/2 loss=0.80204\n",
      "epoch=42 batch=2/2 loss=0.15475\n",
      "epoch=43 batch=1/2 loss=0.33987\n",
      "epoch=43 batch=2/2 loss=0.28430\n",
      "epoch=44 batch=1/2 loss=0.20283\n",
      "epoch=44 batch=2/2 loss=0.25925\n",
      "epoch=45 batch=1/2 loss=0.11578\n",
      "epoch=45 batch=2/2 loss=0.19039\n",
      "epoch=46 batch=1/2 loss=0.12209\n",
      "epoch=46 batch=2/2 loss=0.11008\n",
      "epoch=47 batch=1/2 loss=0.10329\n",
      "epoch=47 batch=2/2 loss=0.08200\n",
      "epoch=48 batch=1/2 loss=0.07731\n",
      "epoch=48 batch=2/2 loss=0.06940\n",
      "epoch=49 batch=1/2 loss=0.06129\n",
      "epoch=49 batch=2/2 loss=0.25642\n",
      "epoch=50 batch=1/2 loss=0.05126\n",
      "epoch=50 batch=2/2 loss=0.05848\n",
      "epoch=51 batch=1/2 loss=0.25072\n",
      "epoch=51 batch=2/2 loss=0.09056\n",
      "epoch=52 batch=1/2 loss=0.05097\n",
      "epoch=52 batch=2/2 loss=0.08885\n",
      "epoch=53 batch=1/2 loss=0.05294\n",
      "epoch=53 batch=2/2 loss=0.07540\n",
      "epoch=54 batch=1/2 loss=0.03704\n",
      "epoch=54 batch=2/2 loss=0.06445\n",
      "epoch=55 batch=1/2 loss=0.02890\n",
      "epoch=55 batch=2/2 loss=0.05373\n",
      "epoch=56 batch=1/2 loss=0.02536\n",
      "epoch=56 batch=2/2 loss=0.05254\n",
      "epoch=57 batch=1/2 loss=0.02241\n",
      "epoch=57 batch=2/2 loss=0.04049\n",
      "epoch=58 batch=1/2 loss=0.01962\n",
      "epoch=58 batch=2/2 loss=0.03488\n",
      "epoch=59 batch=1/2 loss=0.01606\n",
      "epoch=59 batch=2/2 loss=0.05294\n",
      "epoch=60 batch=1/2 loss=0.01475\n",
      "epoch=60 batch=2/2 loss=0.05060\n",
      "epoch=61 batch=1/2 loss=0.01410\n",
      "epoch=61 batch=2/2 loss=0.06587\n",
      "epoch=62 batch=1/2 loss=0.01240\n",
      "epoch=62 batch=2/2 loss=0.05812\n",
      "epoch=63 batch=1/2 loss=0.01154\n",
      "epoch=63 batch=2/2 loss=0.05208\n",
      "epoch=64 batch=1/2 loss=0.01358\n",
      "epoch=64 batch=2/2 loss=0.04211\n",
      "epoch=65 batch=1/2 loss=0.01023\n",
      "epoch=65 batch=2/2 loss=0.04462\n",
      "epoch=66 batch=1/2 loss=0.01024\n",
      "epoch=66 batch=2/2 loss=0.04558\n",
      "epoch=67 batch=1/2 loss=0.00950\n",
      "epoch=67 batch=2/2 loss=0.03899\n",
      "epoch=68 batch=1/2 loss=0.01011\n",
      "epoch=68 batch=2/2 loss=0.04199\n",
      "epoch=69 batch=1/2 loss=0.01452\n",
      "epoch=69 batch=2/2 loss=0.04618\n",
      "epoch=70 batch=1/2 loss=0.00870\n",
      "epoch=70 batch=2/2 loss=0.04064\n",
      "epoch=71 batch=1/2 loss=0.00826\n",
      "epoch=71 batch=2/2 loss=0.03975\n",
      "epoch=72 batch=1/2 loss=0.00802\n",
      "epoch=72 batch=2/2 loss=0.03845\n",
      "epoch=73 batch=1/2 loss=0.00781\n",
      "epoch=73 batch=2/2 loss=0.03734\n",
      "epoch=74 batch=1/2 loss=0.00761\n",
      "epoch=74 batch=2/2 loss=0.03458\n",
      "epoch=75 batch=1/2 loss=0.00714\n",
      "epoch=75 batch=2/2 loss=0.03401\n",
      "epoch=76 batch=1/2 loss=0.00646\n",
      "epoch=76 batch=2/2 loss=0.03498\n",
      "epoch=77 batch=1/2 loss=0.00603\n",
      "epoch=77 batch=2/2 loss=0.03427\n",
      "epoch=78 batch=1/2 loss=0.00576\n",
      "epoch=78 batch=2/2 loss=0.03362\n",
      "epoch=79 batch=1/2 loss=0.00545\n",
      "epoch=79 batch=2/2 loss=0.03302\n",
      "epoch=80 batch=1/2 loss=0.00524\n",
      "epoch=80 batch=2/2 loss=0.03246\n",
      "epoch=81 batch=1/2 loss=0.00507\n",
      "epoch=81 batch=2/2 loss=0.03193\n",
      "epoch=82 batch=1/2 loss=0.00488\n",
      "epoch=82 batch=2/2 loss=0.03149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8156\\95078711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msave_every_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlog_every_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32md:\\Repos\\Other\\slap\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, save_every_n, log_every_n, batch_size, run_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_every_n\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\Other\\slap\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, entry)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mspectros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspectros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspectros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\hambone\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Repos\\Other\\slap\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mel_spectro)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\hambone\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\hambone\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\hambone\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    303\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[1;32m--> 304\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    epochs=100,\n",
    "    batch_size=200,\n",
    "    save_every_n=10,\n",
    "    log_every_n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint 172\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Other\\slap\\dataset.py:56: FutureWarning: Pass size=3379200 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  audio = librosa.util.fix_length(audio, full_length)\n",
      "d:\\Repos\\Other\\slap\\dataset.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  return label, torch.tensor(spectros)\n"
     ]
    }
   ],
   "source": [
    "y, x = ds[0]\n",
    "y_pred = model(x[50:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['bad apple'], dtype='<U13')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print([ds.label_encoder.inverse_transform([np.argmax(y)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'bad apple': 62, 'family affair': 12, 'sky magic': 8})\n",
      "bad apple\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "y_pred_lbl = list(ds.label_encoder.inverse_transform(y_pred.argmax(axis=1)))\n",
    "print(Counter(y_pred_lbl))\n",
    "print(max(y_pred_lbl, key=y_pred_lbl.count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hambone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b05b3b44eb53e84f110eb6a4a3319032819c2303fde86f693bd6476536004c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
